{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from data import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "def discriminator_bad(dim_length,d=32,input_shape=(64,64,1),name='discriminator',output_dim=10):\n",
    "    dims=[d *(2**i) for i in range(2,2+dim_length)]\n",
    "    img_inputs = keras.Input(shape=(input_shape))\n",
    "    x = layers.Conv2D(d, (4, 4), strides=(2, 2), padding=\"same\",kernel_initializer=initializer)(img_inputs)\n",
    "    x=layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    for dim in dims:\n",
    "        x = layers.Conv2D(d, (4, 4), strides=(2, 2), padding=\"same\",kernel_initializer=initializer)(x)\n",
    "        x= layers.BatchNormalization()(x)\n",
    "        x=layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    binary_x =layers.Conv2D(d, (4,4),strides=(2,2),kernel_initializer=initializer)(x)\n",
    "    binary_x=layers.Flatten(name='binary_flatten')(binary_x)\n",
    "    binary_output = layers.Dense(1,name='binary_output')(binary_x)\n",
    "\n",
    "    multiclass_x=layers.Dense(d,kernel_initializer=initializer)(x)\n",
    "    mutliclass_x=layers.Conv2D(dims[-1], (4,4),strides=(2,2),kernel_initializer=initializer)(multiclass_x)\n",
    "    multiclass_x=layers.Flatten(name='multiclass_flatten')(multiclass_x)\n",
    "    mutlticlass_output=layers.Dense(output_dim,name='multiclass_output')(multiclass_x)\n",
    "    model = keras.Model(inputs=img_inputs, outputs=[binary_output,mutlticlass_output], name=name)\n",
    "    return model\n",
    "\n",
    "def discriminator(input_shape=(64,64,1),name='discriminator',output_dim=10):\n",
    "    img_inputs = keras.Input(shape=(input_shape))\n",
    "    d=input_shape[0]\n",
    "    #x=layers.Flatten()(img_inputs)\n",
    "    x=layers.Conv2D(16, (4, 4), strides=(2, 2), padding=\"same\")(img_inputs)\n",
    "    x=layers.LeakyReLU(alpha=.2)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x=layers.Conv2D(32, (4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU(alpha=.2)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x=layers.Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "    x=layers.LeakyReLU(alpha=.2)(x)\n",
    "    x= layers.BatchNormalization()(x)\n",
    "    x=layers.Flatten()(x)\n",
    "    #binary_x=layers.Flatten(name='binary_flatten')(x)\n",
    "    binary_output = layers.Dense(1,name='binary_output')(x)\n",
    "    #multiclass_x=layers.Dense(,kernel_initializer=initializer)(x)\n",
    "    #multiclass_x=layers.Flatten(name='multiclass_flatten')(multiclass_x)\n",
    "    mutlticlass_output=layers.Dense(output_dim,name='multiclass_output')(x)\n",
    "    #model = keras.Model(inputs=img_inputs, outputs=[binary_output,mutlticlass_output], name=name)\n",
    "    model =keras.Model(inputs=img_inputs, outputs=binary_output, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^The above functions define a discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 24.73it/s]\n"
     ]
    }
   ],
   "source": [
    "d=dataset_limited(['cubism'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset_limited is defined in data.py; it just gets the feature maps (represented as numpy arrays) for the artistic genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dick=discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 16)        272       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 16, 16, 32)        8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 64)          32832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "binary_output (Dense)        (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 45,873\n",
      "Trainable params: 45,649\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dick.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator(latent_dim = 128):\n",
    "    gen_1=keras.Sequential([\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "            # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
    "            layers.Dense(2*latent_dim),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Reshape((8, 8, int(2*latent_dim/64))),\n",
    "            layers.Conv2DTranspose(latent_dim, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Conv2DTranspose(latent_dim/2, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Conv2DTranspose(latent_dim/4, (2, 2), strides=(2, 2), padding=\"same\"),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.Conv2D(1, (2,2), padding=\"same\"),\n",
    "            layers.BatchNormalization(name=style_blocks[0])\n",
    "    ],name='gen_1')\n",
    "    gen_2=keras.Sequential([\n",
    "      keras.Input(shape=(latent_dim,)),\n",
    "            gen_1,\n",
    "            layers.Dense(64),\n",
    "        layers.Conv2DTranspose(64,(2,2),strides=(2,2),padding='same'),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(),  \n",
    "        layers.Conv2D(1,(2,2),padding='same'),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(name=style_blocks[1]),  \n",
    "    ],name='gen_2')\n",
    "    gen_3=keras.Sequential([\n",
    "      keras.Input(shape=(latent_dim,)),\n",
    "            gen_2,\n",
    "            layers.UpSampling2D(),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(name=style_blocks[2]),  \n",
    "    ],name='gen_3')\n",
    "    gen_4=keras.Sequential([\n",
    "      keras.Input(shape=(latent_dim,)),\n",
    "            gen_3,\n",
    "            layers.UpSampling2D(),\n",
    "            layers.LeakyReLU(alpha=0.2),\n",
    "            layers.BatchNormalization(name=style_blocks[3]),  \n",
    "    ],name='gen_4')\n",
    "    gen_5 = keras.Sequential(\n",
    "        [\n",
    "            keras.Input(shape=(latent_dim,)),\n",
    "            gen_4,\n",
    "            layers.Conv2D(1, (2,2), padding=\"same\"),\n",
    "            layers.LeakyReLU(alpha=0.2,name=style_blocks[4])\n",
    "        ],\n",
    "        name=\"gen_5\",\n",
    "    )\n",
    "    return [gen_1,gen_2,gen_3,gen_4,gen_5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 26.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 24.73it/s]\n"
     ]
    }
   ],
   "source": [
    "genres=random_genres(2)\n",
    "labels,matrices=dataset_batched(genres,limit=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random_genres is a function defined in extras.py; it just gets a bunch of random genres. dataset_batched is defined in data.py, it just gets the labels (if a painting is in 'cubism', label =cubism, represented as a one-hot encoding vector) and numpy representations of the paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_zipped=tf.data.Dataset.zip(tuple(m for m in matrices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminators, generators, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminators = discriminators\n",
    "        self.generators = generators\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, binary_loss_fn,multiclass_loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.binary_loss_fn = binary_loss_fn\n",
    "        self.multiclass_loss_fn = multiclass_loss_fn\n",
    "\n",
    "    def train_step(self, labels_multiclass,matrices,batch_size):\n",
    "        #fake images\n",
    "        layer_loss=[]\n",
    "        net_d_loss=0\n",
    "        net_g_loss=0\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        # Decode them to fake images\n",
    "        #generated_images = self.generator(random_latent_vectors)\n",
    "        ret={}\n",
    "        updated_g_vars=0\n",
    "        for real_mats,block,discriminator,generator in zip(matrices,style_blocks,self.discriminators,self.generators):\n",
    "            gen_mats=generator(random_latent_vectors)\n",
    "            combined_mats = tf.concat([gen_mats, real_mats], axis=0)\n",
    "            # Assemble labels discriminating real from fake images\n",
    "            labels_binary = tf.concat(\n",
    "                [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "            )\n",
    "            # Add random noise to the labels - important trick!\n",
    "            labels_binary += 0.05 * tf.random.uniform(tf.shape(labels_binary))\n",
    "            # Train the discriminator\n",
    "            with tf.GradientTape() as tape:\n",
    "                #pred_binary,pred_multiclass = discriminator(combined_mats)\n",
    "                pred_binary= discriminator(combined_mats)\n",
    "                #pred_multiclass=pred_multiclass[-len(real_mats):]\n",
    "                d_loss = self.binary_loss_fn(labels_binary, pred_binary) #+self.multiclass_loss_fn(labels_multiclass,pred_multiclass)\n",
    "            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(grads, discriminator.trainable_weights)\n",
    "            )\n",
    "            \n",
    "            #random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            # Assemble labels that say \"all real images\"\n",
    "            misleading_labels = tf.zeros((batch_size, 1))\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            # Train the generator\n",
    "            #g_mats = tf.Variable([func(vector)[0] for vector in random_latent_vectors])\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = discriminator(generator(random_latent_vectors))\n",
    "                g_loss = self.binary_loss_fn( misleading_labels,predictions)\n",
    "            grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "            #print([sum(grad) for grad in grads])\n",
    "            for g in range(updated_g_vars):\n",
    "                grads[g]=tf.zeros(shape=grads[g].shape)\n",
    "            updated_g_vars=len(grads)\n",
    "            self.g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "            \n",
    "            net_d_loss+=d_loss\n",
    "            net_g_loss+=g_loss\n",
    "            layer_loss.append({'g_loss':g_loss,'d_loss':d_loss})\n",
    "        return layer_loss,net_g_loss,net_d_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defines the GAN with its own training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes=[(64, 64,1), (128, 128,1), (256, 256,1), (512, 512, 1), (512, 512,1)]\n",
    "discriminators=[discriminator(input_shape=s,name=str(s),output_dim=len(genres)) for s in shapes]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five discriminators, since we're using feature maps from 5 different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"(64, 64, 1)\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64, 64, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 16)        272       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 32, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 32)        8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 64)          32832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "binary_output (Dense)        (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 45,873\n",
      "Trainable params: 45,649\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "Model: \"(128, 128, 1)\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 16)        272       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 32)        8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 64)        32832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "binary_output (Dense)        (None, 1)                 16385     \n",
      "=================================================================\n",
      "Total params: 58,161\n",
      "Trainable params: 57,937\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "Model: \"(256, 256, 1)\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 256, 256, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 128, 128, 16)      272       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 128, 128, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128, 128, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 32)        8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 64)        32832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 65536)             0         \n",
      "_________________________________________________________________\n",
      "binary_output (Dense)        (None, 1)                 65537     \n",
      "=================================================================\n",
      "Total params: 107,313\n",
      "Trainable params: 107,089\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "Model: \"(512, 512, 1)\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 512, 512, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 256, 256, 16)      272       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 256, 256, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 256, 256, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 128, 128, 32)      8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 64, 64, 64)        32832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 262144)            0         \n",
      "_________________________________________________________________\n",
      "binary_output (Dense)        (None, 1)                 262145    \n",
      "=================================================================\n",
      "Total params: 303,921\n",
      "Trainable params: 303,697\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "Model: \"(512, 512, 1)\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 512, 512, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 256, 256, 16)      272       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 256, 256, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 256, 256, 16)      64        \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 128, 128, 32)      8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 64, 64, 64)        32832     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 262144)            0         \n",
      "_________________________________________________________________\n",
      "binary_output (Dense)        (None, 1)                 262145    \n",
      "=================================================================\n",
      "Total params: 303,921\n",
      "Trainable params: 303,697\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for d in discriminators:\n",
    "    d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"gen_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 16, 16, 128)       8320      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 64)        131136    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 64, 64, 32)        8224      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 64, 64, 1)         129       \n",
      "_________________________________________________________________\n",
      "block1_conv1 (BatchNormaliza (None, 64, 64, 1)         4         \n",
      "=================================================================\n",
      "Total params: 181,861\n",
      "Trainable params: 181,347\n",
      "Non-trainable params: 514\n",
      "_________________________________________________________________\n",
      "Model: \"gen_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gen_1 (Sequential)           (None, 64, 64, 1)         181861    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64, 64, 64)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 128, 128, 64)      16448     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 128, 128, 1)       257       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (BatchNormaliza (None, 128, 128, 1)       4         \n",
      "=================================================================\n",
      "Total params: 198,954\n",
      "Trainable params: 198,310\n",
      "Non-trainable params: 644\n",
      "_________________________________________________________________\n",
      "Model: \"gen_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gen_2 (Sequential)           (None, 128, 128, 1)       198954    \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (BatchNormaliza (None, 256, 256, 1)       4         \n",
      "=================================================================\n",
      "Total params: 198,958\n",
      "Trainable params: 198,312\n",
      "Non-trainable params: 646\n",
      "_________________________________________________________________\n",
      "Model: \"gen_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gen_3 (Sequential)           (None, 256, 256, 1)       198958    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 512, 512, 1)       0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 512, 512, 1)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (BatchNormaliza (None, 512, 512, 1)       4         \n",
      "=================================================================\n",
      "Total params: 198,962\n",
      "Trainable params: 198,314\n",
      "Non-trainable params: 648\n",
      "_________________________________________________________________\n",
      "Model: \"gen_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gen_4 (Sequential)           (None, 512, 512, 1)       198962    \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 512, 512, 1)       5         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (LeakyReLU)     (None, 512, 512, 1)       0         \n",
      "=================================================================\n",
      "Total params: 198,967\n",
      "Trainable params: 198,319\n",
      "Non-trainable params: 648\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generators=make_generator()\n",
    "for g in generators:\n",
    "    g.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(discriminators=discriminators, generators=generators, latent_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.00003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.003),\n",
    "    binary_loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    multiclass_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1dc170a8e50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.generators[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start epoch 0\n",
      "step 0: d_loss =7948.01953125 g_loss =3.4659743309020996\n",
      "step 1: d_loss =6285.1318359375 g_loss =3.3944640159606934\n",
      "step 2: d_loss =9203.607421875 g_loss =2.836118221282959\n",
      "step 3: d_loss =10525.9306640625 g_loss =1.1404080390930176\n",
      "step 4: d_loss =18563.7734375 g_loss =0.20400738716125488\n",
      "step 5: d_loss =7316.99267578125 g_loss =0.003850023029372096\n",
      "step 6: d_loss =14144.1943359375 g_loss =0.0025097436737269163\n",
      "step 7: d_loss =7895.74169921875 g_loss =0.0006103832856751978\n",
      "step 8: d_loss =15418.734375 g_loss =0.013675820082426071\n",
      "step 9: d_loss =14552.529296875 g_loss =0.0006455593393184245\n",
      "\n",
      "Start epoch 1\n",
      "step 0: d_loss =15257.9755859375 g_loss =0.0001518892531748861\n",
      "step 1: d_loss =11341.88671875 g_loss =0.0011196843115612864\n",
      "step 2: d_loss =7773.43798828125 g_loss =0.005198971834033728\n",
      "step 3: d_loss =34193.75390625 g_loss =1.0333571434020996\n",
      "step 4: d_loss =8868.2099609375 g_loss =12.710817337036133\n",
      "step 5: d_loss =9712.7744140625 g_loss =24.917949676513672\n",
      "step 6: d_loss =21776.810546875 g_loss =26.03205108642578\n",
      "step 7: d_loss =10722.7041015625 g_loss =25.346576690673828\n",
      "step 8: d_loss =13620.8408203125 g_loss =23.49562644958496\n",
      "step 9: d_loss =20278.595703125 g_loss =26.97046661376953\n",
      "\n",
      "Start epoch 2\n",
      "step 0: d_loss =18199.9765625 g_loss =27.331253051757812\n",
      "step 1: d_loss =25872.724609375 g_loss =17.41640853881836\n",
      "step 2: d_loss =20319.263671875 g_loss =14.23802661895752\n",
      "step 3: d_loss =32545.3125 g_loss =11.421299934387207\n",
      "step 4: d_loss =27148.17578125 g_loss =12.203108787536621\n",
      "step 5: d_loss =8728.8076171875 g_loss =9.841789245605469\n",
      "step 6: d_loss =21891.642578125 g_loss =13.406437873840332\n",
      "step 7: d_loss =6770.853515625 g_loss =13.232067108154297\n",
      "step 8: d_loss =17533.220703125 g_loss =15.424927711486816\n",
      "step 9: d_loss =11491.734375 g_loss =17.036113739013672\n",
      "\n",
      "Start epoch 3\n",
      "step 0: d_loss =18486.42578125 g_loss =15.75969123840332\n",
      "step 1: d_loss =14134.77734375 g_loss =16.630062103271484\n",
      "step 2: d_loss =15184.5419921875 g_loss =18.554912567138672\n",
      "step 3: d_loss =18309.853515625 g_loss =15.739140510559082\n",
      "step 4: d_loss =13743.9296875 g_loss =6.221297264099121\n",
      "step 5: d_loss =3446.1796875 g_loss =2.1016650199890137\n",
      "step 6: d_loss =6302.19677734375 g_loss =2.159350872039795\n",
      "step 7: d_loss =6330.18115234375 g_loss =2.5047054290771484\n",
      "step 8: d_loss =12914.6015625 g_loss =1.5270817279815674\n",
      "step 9: d_loss =10476.517578125 g_loss =0.608410656452179\n",
      "\n",
      "Start epoch 4\n",
      "step 0: d_loss =11270.8408203125 g_loss =0.17459002137184143\n",
      "step 1: d_loss =13227.09375 g_loss =0.27377626299858093\n",
      "step 2: d_loss =7711.56640625 g_loss =0.4793890118598938\n",
      "step 3: d_loss =25734.611328125 g_loss =0.7332146763801575\n",
      "step 4: d_loss =23965.365234375 g_loss =0.744256317615509\n",
      "step 5: d_loss =4451.93359375 g_loss =1.6221997737884521\n",
      "step 6: d_loss =12211.822265625 g_loss =3.9151833057403564\n",
      "step 7: d_loss =6816.37890625 g_loss =3.7391796112060547\n",
      "step 8: d_loss =6838.19873046875 g_loss =7.169427871704102\n",
      "step 9: d_loss =8728.35546875 g_loss =9.485614776611328\n",
      "\n",
      "Start epoch 5\n",
      "step 0: d_loss =9921.5771484375 g_loss =10.005668640136719\n",
      "step 1: d_loss =10389.234375 g_loss =10.30264949798584\n",
      "step 2: d_loss =9136.361328125 g_loss =12.39389419555664\n",
      "step 3: d_loss =16779.587890625 g_loss =11.151374816894531\n",
      "step 4: d_loss =6794.8125 g_loss =14.116395950317383\n",
      "step 5: d_loss =3244.9677734375 g_loss =12.538101196289062\n",
      "step 6: d_loss =4718.7099609375 g_loss =11.65310287475586\n",
      "step 7: d_loss =3983.575927734375 g_loss =10.975374221801758\n",
      "step 8: d_loss =5271.22705078125 g_loss =9.478093147277832\n",
      "step 9: d_loss =11592.654296875 g_loss =9.515507698059082\n",
      "\n",
      "Start epoch 6\n",
      "step 0: d_loss =8440.455078125 g_loss =10.33080768585205\n",
      "step 1: d_loss =5146.353515625 g_loss =8.339305877685547\n",
      "step 2: d_loss =9351.1748046875 g_loss =9.886423110961914\n",
      "step 3: d_loss =15435.5 g_loss =10.915836334228516\n",
      "step 4: d_loss =9603.3447265625 g_loss =8.95154094696045\n",
      "step 5: d_loss =2406.23046875 g_loss =8.240737915039062\n",
      "step 6: d_loss =2718.144775390625 g_loss =8.576759338378906\n",
      "step 7: d_loss =2093.910400390625 g_loss =9.135884284973145\n",
      "step 8: d_loss =6516.92724609375 g_loss =9.103466033935547\n",
      "step 9: d_loss =13192.380859375 g_loss =5.4731268882751465\n",
      "\n",
      "Start epoch 7\n",
      "step 0: d_loss =4711.87451171875 g_loss =7.579063415527344\n",
      "step 1: d_loss =7092.2314453125 g_loss =4.955328464508057\n",
      "step 2: d_loss =4681.5498046875 g_loss =5.393304824829102\n",
      "step 3: d_loss =9597.880859375 g_loss =2.1378273963928223\n",
      "step 4: d_loss =6124.837890625 g_loss =0.7417805194854736\n",
      "step 5: d_loss =4966.10205078125 g_loss =0.645564079284668\n",
      "step 6: d_loss =4533.33056640625 g_loss =0.7276069521903992\n",
      "step 7: d_loss =4167.369140625 g_loss =0.722556471824646\n",
      "step 8: d_loss =7965.705078125 g_loss =0.7648300528526306\n",
      "step 9: d_loss =7025.5224609375 g_loss =0.6262195706367493\n",
      "\n",
      "Start epoch 8\n",
      "step 0: d_loss =6388.09765625 g_loss =0.5232184529304504\n",
      "step 1: d_loss =1915.1080322265625 g_loss =0.39598193764686584\n",
      "step 2: d_loss =5777.4873046875 g_loss =0.4208337664604187\n",
      "step 3: d_loss =8122.69775390625 g_loss =0.42066383361816406\n",
      "step 4: d_loss =7891.3984375 g_loss =0.5568982362747192\n",
      "step 5: d_loss =6670.4560546875 g_loss =0.5675129294395447\n",
      "step 6: d_loss =8816.5087890625 g_loss =0.6538204550743103\n",
      "step 7: d_loss =6777.77294921875 g_loss =0.6499749422073364\n",
      "step 8: d_loss =14252.0087890625 g_loss =0.9698200821876526\n",
      "step 9: d_loss =14709.66015625 g_loss =0.8880894184112549\n",
      "\n",
      "Start epoch 9\n",
      "step 0: d_loss =8675.078125 g_loss =0.8961395025253296\n",
      "step 1: d_loss =9971.98046875 g_loss =1.348774790763855\n",
      "step 2: d_loss =7954.654296875 g_loss =0.9643762707710266\n",
      "step 3: d_loss =9742.107421875 g_loss =1.4308743476867676\n",
      "step 4: d_loss =5147.67041015625 g_loss =1.8794023990631104\n",
      "step 5: d_loss =1603.7349853515625 g_loss =0.9547320008277893\n",
      "step 6: d_loss =5760.009765625 g_loss =1.0159668922424316\n",
      "step 7: d_loss =5035.9267578125 g_loss =1.3373311758041382\n",
      "step 8: d_loss =5886.921875 g_loss =1.3805454969406128\n",
      "step 9: d_loss =6210.5380859375 g_loss =1.5934983491897583\n",
      "\n",
      "Start epoch 10\n",
      "step 0: d_loss =4812.283203125 g_loss =1.5930731296539307\n",
      "step 1: d_loss =5673.16845703125 g_loss =1.4611915349960327\n",
      "step 2: d_loss =7754.822265625 g_loss =1.3976731300354004\n",
      "step 3: d_loss =4616.35595703125 g_loss =1.361398458480835\n",
      "step 4: d_loss =16499.951171875 g_loss =0.9847062230110168\n",
      "step 5: d_loss =7394.44775390625 g_loss =0.9561471939086914\n",
      "step 6: d_loss =16393.13671875 g_loss =0.8736890554428101\n",
      "step 7: d_loss =11303.3916015625 g_loss =0.9387908577919006\n",
      "step 8: d_loss =12705.3310546875 g_loss =1.7270865440368652\n",
      "step 9: d_loss =13053.1318359375 g_loss =1.311396598815918\n",
      "\n",
      "Start epoch 11\n",
      "step 0: d_loss =14593.8671875 g_loss =0.9335078597068787\n",
      "step 1: d_loss =15549.4775390625 g_loss =1.5079896450042725\n",
      "step 2: d_loss =14811.7197265625 g_loss =1.3724687099456787\n",
      "step 3: d_loss =16049.9267578125 g_loss =1.2786839008331299\n",
      "step 4: d_loss =15236.0517578125 g_loss =1.0749733448028564\n",
      "step 5: d_loss =9101.7548828125 g_loss =1.1032925844192505\n",
      "step 6: d_loss =7738.501953125 g_loss =1.8923699855804443\n",
      "step 7: d_loss =7181.54248046875 g_loss =1.8962903022766113\n",
      "step 8: d_loss =9266.74609375 g_loss =2.261253833770752\n",
      "step 9: d_loss =7498.6953125 g_loss =3.2298033237457275\n",
      "\n",
      "Start epoch 12\n",
      "step 0: d_loss =8041.5830078125 g_loss =3.014430284500122\n",
      "step 1: d_loss =6208.52783203125 g_loss =4.8849358558654785\n",
      "step 2: d_loss =4370.97216796875 g_loss =2.922175407409668\n",
      "step 3: d_loss =12611.900390625 g_loss =2.771299123764038\n",
      "step 4: d_loss =5866.68310546875 g_loss =2.890481948852539\n",
      "step 5: d_loss =3708.041748046875 g_loss =3.1298234462738037\n",
      "step 6: d_loss =7225.63232421875 g_loss =4.753759860992432\n",
      "step 7: d_loss =3563.159912109375 g_loss =6.254837512969971\n",
      "step 8: d_loss =5642.35302734375 g_loss =5.014804363250732\n",
      "step 9: d_loss =7108.52392578125 g_loss =2.9178221225738525\n",
      "\n",
      "Start epoch 13\n",
      "step 0: d_loss =5413.8408203125 g_loss =3.0582990646362305\n",
      "step 1: d_loss =6075.51416015625 g_loss =2.4288687705993652\n",
      "step 2: d_loss =6323.50732421875 g_loss =2.776029109954834\n",
      "step 3: d_loss =6148.97412109375 g_loss =4.6344146728515625\n",
      "step 4: d_loss =9457.798828125 g_loss =4.102701187133789\n",
      "step 5: d_loss =3974.446044921875 g_loss =9.15829086303711\n",
      "step 6: d_loss =12986.6611328125 g_loss =6.845253944396973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7: d_loss =2386.109375 g_loss =8.631892204284668\n",
      "step 8: d_loss =21823.99609375 g_loss =9.077024459838867\n",
      "step 9: d_loss =11137.470703125 g_loss =4.801805019378662\n",
      "\n",
      "Start epoch 14\n",
      "step 0: d_loss =3636.5126953125 g_loss =5.887483596801758\n",
      "step 1: d_loss =5331.193359375 g_loss =1.1192866563796997\n",
      "step 2: d_loss =6407.94140625 g_loss =5.529237270355225\n",
      "step 3: d_loss =8887.9931640625 g_loss =2.270313262939453\n",
      "step 4: d_loss =16951.572265625 g_loss =2.005033016204834\n",
      "step 5: d_loss =6980.1318359375 g_loss =1.5734823942184448\n",
      "step 6: d_loss =15432.7998046875 g_loss =4.27389669418335\n",
      "step 7: d_loss =8357.404296875 g_loss =1.2536100149154663\n",
      "step 8: d_loss =9332.6484375 g_loss =4.7569427490234375\n",
      "step 9: d_loss =15348.3955078125 g_loss =5.7549943923950195\n",
      "\n",
      "Start epoch 15\n",
      "step 0: d_loss =11237.6357421875 g_loss =6.0348968505859375\n",
      "step 1: d_loss =20589.08984375 g_loss =8.094359397888184\n",
      "step 2: d_loss =9133.220703125 g_loss =7.301883220672607\n",
      "step 3: d_loss =16347.6123046875 g_loss =8.543083190917969\n",
      "step 4: d_loss =12887.51171875 g_loss =8.317761421203613\n",
      "step 5: d_loss =8783.740234375 g_loss =7.548437595367432\n",
      "step 6: d_loss =19280.919921875 g_loss =5.759819984436035\n",
      "step 7: d_loss =12601.353515625 g_loss =3.7753794193267822\n",
      "step 8: d_loss =15366.05859375 g_loss =4.039981365203857\n",
      "step 9: d_loss =16351.0869140625 g_loss =3.936898946762085\n",
      "\n",
      "Start epoch 16\n",
      "step 0: d_loss =15437.0 g_loss =4.660105228424072\n",
      "step 1: d_loss =13167.236328125 g_loss =2.682882785797119\n",
      "step 2: d_loss =11245.513671875 g_loss =4.260400772094727\n",
      "step 3: d_loss =24253.630859375 g_loss =9.283032417297363\n",
      "step 4: d_loss =12542.8525390625 g_loss =10.921710014343262\n",
      "step 5: d_loss =6560.7236328125 g_loss =16.035938262939453\n",
      "step 6: d_loss =10955.1806640625 g_loss =13.802189826965332\n",
      "step 7: d_loss =3304.669677734375 g_loss =6.065669536590576\n",
      "step 8: d_loss =7069.8701171875 g_loss =1.3329625129699707\n",
      "step 9: d_loss =12688.935546875 g_loss =1.8824875354766846\n",
      "\n",
      "Start epoch 17\n",
      "step 0: d_loss =5297.4970703125 g_loss =2.3898708820343018\n",
      "step 1: d_loss =5027.17529296875 g_loss =2.982496500015259\n",
      "step 2: d_loss =15465.2978515625 g_loss =2.5982089042663574\n",
      "step 3: d_loss =8554.5771484375 g_loss =3.2836060523986816\n",
      "step 4: d_loss =7320.27099609375 g_loss =3.236006498336792\n",
      "step 5: d_loss =1934.7764892578125 g_loss =2.8533883094787598\n",
      "step 6: d_loss =6075.787109375 g_loss =3.220661163330078\n",
      "step 7: d_loss =3982.86083984375 g_loss =3.880413055419922\n",
      "step 8: d_loss =9977.423828125 g_loss =3.838874578475952\n",
      "step 9: d_loss =4434.94873046875 g_loss =4.497793197631836\n",
      "\n",
      "Start epoch 18\n",
      "step 0: d_loss =8713.736328125 g_loss =5.175047874450684\n",
      "step 1: d_loss =8336.287109375 g_loss =4.905487537384033\n",
      "step 2: d_loss =8780.1796875 g_loss =5.287162780761719\n",
      "step 3: d_loss =13624.78515625 g_loss =3.810580015182495\n",
      "step 4: d_loss =11765.828125 g_loss =3.3382909297943115\n",
      "step 5: d_loss =4108.1708984375 g_loss =2.857602596282959\n",
      "step 6: d_loss =4625.60595703125 g_loss =3.1642940044403076\n",
      "step 7: d_loss =5391.6962890625 g_loss =2.692845582962036\n",
      "step 8: d_loss =7522.41064453125 g_loss =2.6582276821136475\n",
      "step 9: d_loss =6062.5419921875 g_loss =2.917922258377075\n",
      "\n",
      "Start epoch 19\n",
      "step 0: d_loss =5686.6640625 g_loss =2.737122058868408\n",
      "step 1: d_loss =7178.4970703125 g_loss =2.7018065452575684\n",
      "step 2: d_loss =4032.650634765625 g_loss =2.578190326690674\n",
      "step 3: d_loss =4450.29296875 g_loss =2.6730828285217285\n",
      "step 4: d_loss =7140.12060546875 g_loss =2.4800355434417725\n",
      "step 5: d_loss =2577.87548828125 g_loss =2.634107828140259\n",
      "step 6: d_loss =5267.31689453125 g_loss =2.5107264518737793\n",
      "step 7: d_loss =2412.551513671875 g_loss =2.664290189743042\n",
      "step 8: d_loss =4721.95458984375 g_loss =2.548762321472168\n",
      "step 9: d_loss =6086.8193359375 g_loss =2.388601541519165\n"
     ]
    }
   ],
   "source": [
    "epochs=20\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart epoch\", epoch)\n",
    "\n",
    "    step=0\n",
    "    for step,(l,m) in enumerate(zip(labels,matrices_zipped)):\n",
    "        # Train the discriminator & generator on one batch of real images.\n",
    "        layer_loss,net_g_loss,net_d_loss= gan.train_step(l,m,10)\n",
    "        print(\"step {}: d_loss ={} g_loss ={}\".format(step,net_d_loss,net_g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimizers(d_optimizer,g_optimizer,epochs=10,GAN=GAN):\n",
    "    discriminators=[discriminator(input_shape=s,name=str(s),output_dim=len(genres)) for s in shapes]\n",
    "    generators=make_generator()\n",
    "    gan = GAN(discriminators=discriminators, generators=generators, latent_dim=128)\n",
    "    gan.compile(\n",
    "        d_optimizer=d_optimizer,\n",
    "        g_optimizer=g_optimizer,\n",
    "        binary_loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        multiclass_loss_fn=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    )\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart epoch\", epoch)\n",
    "\n",
    "        step=0\n",
    "        for step,(l,m) in enumerate(zip(labels,matrices_zipped)):\n",
    "            # Train the discriminator & generator on one batch of real images.\n",
    "            layer_loss,net_g_loss,net_d_loss= gan.train_step(l,m,10)\n",
    "            print(\"step {}: d_loss ={} g_loss ={}\".format(step,net_d_loss,net_g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start epoch 0\n",
      "step 0: d_loss =6293.24755859375 g_loss =3.4095864295959473\n",
      "step 1: d_loss =12484892.0 g_loss =3.551870822906494\n",
      "step 2: d_loss =56538.015625 g_loss =3.2099716663360596\n",
      "step 3: d_loss =118615.2734375 g_loss =14.848846435546875\n",
      "step 4: d_loss =370862.15625 g_loss =69.11424255371094\n",
      "step 5: d_loss =4257733.5 g_loss =12.385921478271484\n",
      "step 6: d_loss =388834.875 g_loss =118.61106872558594\n",
      "step 7: d_loss =590780.375 g_loss =6.285704612731934\n",
      "step 8: d_loss =674149.5625 g_loss =91.84060668945312\n",
      "step 9: d_loss =858037.25 g_loss =97.4207534790039\n",
      "\n",
      "Start epoch 1\n",
      "step 0: d_loss =1073855.0 g_loss =66.67455291748047\n",
      "step 1: d_loss =308587.375 g_loss =56.098487854003906\n",
      "step 2: d_loss =1063334.25 g_loss =38.314605712890625\n",
      "step 3: d_loss =2583110.0 g_loss =29.885940551757812\n",
      "step 4: d_loss =2283411.0 g_loss =42.5042724609375\n",
      "step 5: d_loss =858156.8125 g_loss =120.31988525390625\n",
      "step 6: d_loss =197185.0625 g_loss =281.8813171386719\n",
      "step 7: d_loss =12115778.0 g_loss =64.13897705078125\n",
      "step 8: d_loss =534006.75 g_loss =57.409645080566406\n",
      "step 9: d_loss =1624945.25 g_loss =0.21240386366844177\n",
      "\n",
      "Start epoch 2\n",
      "step 0: d_loss =1143738.375 g_loss =0.2923688292503357\n",
      "step 1: d_loss =1766950.875 g_loss =0.5107709169387817\n",
      "step 2: d_loss =1488861.375 g_loss =0.6992260217666626\n",
      "step 3: d_loss =1453810.5 g_loss =136.89576721191406\n",
      "step 4: d_loss =1089724.125 g_loss =470.1614990234375\n",
      "step 5: d_loss =344601.375 g_loss =930.1467895507812\n",
      "step 6: d_loss =33834624.0 g_loss =5.300604343414307\n",
      "step 7: d_loss =694586.5 g_loss =68.30743408203125\n",
      "step 8: d_loss =1287411.25 g_loss =459.1723327636719\n",
      "step 9: d_loss =1363599.625 g_loss =1082.80322265625\n",
      "\n",
      "Start epoch 3\n",
      "step 0: d_loss =1356682.875 g_loss =1888.266357421875\n",
      "step 1: d_loss =1170266.75 g_loss =1351.14404296875\n",
      "step 2: d_loss =27497996.0 g_loss =0.04706282168626785\n",
      "step 3: d_loss =3608978.25 g_loss =0.04528147727251053\n",
      "step 4: d_loss =8208948.5 g_loss =0.031627342104911804\n",
      "step 5: d_loss =7946066.0 g_loss =0.07532957196235657\n",
      "step 6: d_loss =10445983.0 g_loss =0.014396199956536293\n",
      "step 7: d_loss =4326115.5 g_loss =0.22872018814086914\n",
      "step 8: d_loss =5740196.0 g_loss =0.035996902734041214\n",
      "step 9: d_loss =3126595.0 g_loss =7.86775016784668\n",
      "\n",
      "Start epoch 4\n",
      "step 0: d_loss =1513131.0 g_loss =454.1882019042969\n",
      "step 1: d_loss =1401236.125 g_loss =815.9963989257812\n",
      "step 2: d_loss =493938.71875 g_loss =1565.0224609375\n",
      "step 3: d_loss =3241739.25 g_loss =1.612278699874878\n",
      "step 4: d_loss =3289396.75 g_loss =1.5855391025543213\n",
      "step 5: d_loss =6302412.0 g_loss =3.135756015777588\n",
      "step 6: d_loss =16140532.0 g_loss =0.10736258327960968\n",
      "step 7: d_loss =7725699.0 g_loss =470.37884521484375\n",
      "step 8: d_loss =8758517.0 g_loss =2070.150146484375\n",
      "step 9: d_loss =7157589.0 g_loss =3300.583740234375\n",
      "\n",
      "Start epoch 5\n",
      "step 0: d_loss =3256679.75 g_loss =8018.482421875\n",
      "step 1: d_loss =27610502.0 g_loss =526.5119018554688\n",
      "step 2: d_loss =4974967.5 g_loss =104.14459228515625\n",
      "step 3: d_loss =12272953.0 g_loss =4.836706638336182\n",
      "step 4: d_loss =12294177.0 g_loss =275.9604797363281\n",
      "step 5: d_loss =14862303.0 g_loss =238.76625061035156\n",
      "step 6: d_loss =20284868.0 g_loss =2531.037109375\n",
      "step 7: d_loss =10114135.0 g_loss =3723.056396484375\n",
      "step 8: d_loss =14313320.0 g_loss =3999.137939453125\n",
      "step 9: d_loss =13962995.0 g_loss =4716.50390625\n",
      "\n",
      "Start epoch 6\n",
      "step 0: d_loss =6104665.5 g_loss =4492.3037109375\n",
      "step 1: d_loss =1473837.5 g_loss =5275.8232421875\n",
      "step 2: d_loss =249937664.0 g_loss =6721.0771484375\n",
      "step 3: d_loss =11034773.0 g_loss =8178.3681640625\n",
      "step 4: d_loss =22561142.0 g_loss =8775.953125\n",
      "step 5: d_loss =7950302.0 g_loss =7127.1943359375\n",
      "step 6: d_loss =18533888.0 g_loss =6266.33837890625\n",
      "step 7: d_loss =7821497.0 g_loss =4313.04345703125\n",
      "step 8: d_loss =6755269.5 g_loss =3070.684814453125\n",
      "step 9: d_loss =142787472.0 g_loss =8005.39208984375\n",
      "\n",
      "Start epoch 7\n",
      "step 0: d_loss =22634572.0 g_loss =12849.9580078125\n",
      "step 1: d_loss =31617870.0 g_loss =17292.53515625\n",
      "step 2: d_loss =48095208.0 g_loss =22488.720703125\n",
      "step 3: d_loss =86467344.0 g_loss =26246.96484375\n",
      "step 4: d_loss =103941000.0 g_loss =29376.28515625\n",
      "step 5: d_loss =67047784.0 g_loss =33372.859375\n",
      "step 6: d_loss =83855288.0 g_loss =33266.09375\n",
      "step 7: d_loss =65229152.0 g_loss =36831.26171875\n",
      "step 8: d_loss =66192404.0 g_loss =36848.921875\n",
      "step 9: d_loss =39630964.0 g_loss =38567.015625\n",
      "\n",
      "Start epoch 8\n",
      "step 0: d_loss =17777856.0 g_loss =37960.125\n",
      "step 1: d_loss =1462801.25 g_loss =36910.90625\n",
      "step 2: d_loss =754953280.0 g_loss =49127.23046875\n",
      "step 3: d_loss =21660482.0 g_loss =62769.5078125\n",
      "step 4: d_loss =47003916.0 g_loss =72241.171875\n",
      "step 5: d_loss =32118650.0 g_loss =88800.34375\n",
      "step 6: d_loss =52235712.0 g_loss =107881.15625\n",
      "step 7: d_loss =18701552.0 g_loss =122994.7890625\n",
      "step 8: d_loss =876801536.0 g_loss =89696.9765625\n",
      "step 9: d_loss =58298148.0 g_loss =65882.515625\n",
      "\n",
      "Start epoch 9\n",
      "step 0: d_loss =90043728.0 g_loss =48030.98828125\n",
      "step 1: d_loss =102190896.0 g_loss =35156.62109375\n",
      "step 2: d_loss =104631336.0 g_loss =26051.52734375\n",
      "step 3: d_loss =138627120.0 g_loss =959.4960327148438\n",
      "step 4: d_loss =137872304.0 g_loss =133.32688903808594\n",
      "step 5: d_loss =102699448.0 g_loss =362.1672058105469\n",
      "step 6: d_loss =147859088.0 g_loss =935.58056640625\n",
      "step 7: d_loss =84047592.0 g_loss =1512.9378662109375\n",
      "step 8: d_loss =57190220.0 g_loss =2778.0126953125\n",
      "step 9: d_loss =33570216.0 g_loss =8367.28515625\n",
      "\n",
      "Start epoch 10\n",
      "step 0: d_loss =12770575.0 g_loss =2.2540574718732387e-05\n",
      "step 1: d_loss =27958738.0 g_loss =989.0739135742188\n",
      "step 2: d_loss =33394304.0 g_loss =2585.40234375\n",
      "step 3: d_loss =59390960.0 g_loss =3817.109130859375\n",
      "step 4: d_loss =60506572.0 g_loss =4236.9560546875\n",
      "step 5: d_loss =52057028.0 g_loss =4847.515625\n",
      "step 6: d_loss =75460736.0 g_loss =5178.744140625\n",
      "step 7: d_loss =43621440.0 g_loss =3768.541259765625\n",
      "step 8: d_loss =44652072.0 g_loss =2692.2763671875\n",
      "step 9: d_loss =13022410.0 g_loss =7620.10693359375\n",
      "\n",
      "Start epoch 11\n",
      "step 0: d_loss =2719132.75 g_loss =15682.494140625\n",
      "step 1: d_loss =665164736.0 g_loss =4125.44921875\n",
      "step 2: d_loss =40301908.0 g_loss =4921.1728515625\n",
      "step 3: d_loss =90583344.0 g_loss =3580.27490234375\n",
      "step 4: d_loss =172624784.0 g_loss =2988.10595703125\n",
      "step 5: d_loss =90493176.0 g_loss =2465.402587890625\n",
      "step 6: d_loss =226239328.0 g_loss =250.3455352783203\n",
      "step 7: d_loss =130753704.0 g_loss =14.153738975524902\n",
      "step 8: d_loss =240896448.0 g_loss =0.4915340840816498\n",
      "step 9: d_loss =244955952.0 g_loss =0.23924854397773743\n",
      "\n",
      "Start epoch 12\n",
      "step 0: d_loss =362811488.0 g_loss =3.3144748210906982\n",
      "step 1: d_loss =222501968.0 g_loss =7.665153503417969\n",
      "step 2: d_loss =138934736.0 g_loss =14.486493110656738\n",
      "step 3: d_loss =288050688.0 g_loss =23.16640281677246\n",
      "step 4: d_loss =230098464.0 g_loss =194.86241149902344\n",
      "step 5: d_loss =132256984.0 g_loss =747.8828125\n",
      "step 6: d_loss =137094672.0 g_loss =1212.7840576171875\n",
      "step 7: d_loss =98512136.0 g_loss =12428.1474609375\n",
      "step 8: d_loss =72690984.0 g_loss =29964.396484375\n",
      "step 9: d_loss =40253448.0 g_loss =46311.296875\n",
      "\n",
      "Start epoch 13\n",
      "step 0: d_loss =475423552.0 g_loss =19524.72265625\n",
      "step 1: d_loss =81955640.0 g_loss =3757.662353515625\n",
      "step 2: d_loss =124389168.0 g_loss =686.4840698242188\n",
      "step 3: d_loss =217752272.0 g_loss =327.8299560546875\n",
      "step 4: d_loss =283589504.0 g_loss =46.907615661621094\n",
      "step 5: d_loss =192815568.0 g_loss =173.5972137451172\n",
      "step 6: d_loss =328409120.0 g_loss =104.35595703125\n",
      "step 7: d_loss =203881280.0 g_loss =3894.357666015625\n",
      "step 8: d_loss =275763328.0 g_loss =20313.55859375\n",
      "step 9: d_loss =241979632.0 g_loss =27430.1484375\n",
      "\n",
      "Start epoch 14\n",
      "step 0: d_loss =277789056.0 g_loss =26984.06640625\n",
      "step 1: d_loss =230361712.0 g_loss =25206.044921875\n",
      "step 2: d_loss =166476336.0 g_loss =17912.287109375\n",
      "step 3: d_loss =256678320.0 g_loss =12747.423828125\n",
      "step 4: d_loss =120751760.0 g_loss =6928.58544921875\n",
      "step 5: d_loss =74827224.0 g_loss =2986.92333984375\n",
      "step 6: d_loss =81990872.0 g_loss =1054.977783203125\n",
      "step 7: d_loss =23361218.0 g_loss =1212.2799072265625\n",
      "step 8: d_loss =303670656.0 g_loss =2919.69921875\n",
      "step 9: d_loss =77716808.0 g_loss =11594.2978515625\n"
     ]
    }
   ],
   "source": [
    "test_optimizers(\n",
    "    keras.optimizers.Adam(learning_rate=0.003,clipnorm=1),keras.optimizers.Adam(learning_rate=0.003,clipnorm=.1),epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start epoch 0\n",
      "step 0: d_loss =4613.5478515625 g_loss =767.9803466796875\n",
      "step 1: d_loss =207150720.0 g_loss =4805.52001953125\n",
      "step 2: d_loss =32683208.0 g_loss =4435.7607421875\n",
      "step 3: d_loss =1786087936.0 g_loss =3773.489013671875\n",
      "step 4: d_loss =374036640.0 g_loss =2550.514404296875\n",
      "step 5: d_loss =84495792.0 g_loss =5352.91650390625\n",
      "step 6: d_loss =24789739520.0 g_loss =14522.79296875\n",
      "step 7: d_loss =743788800.0 g_loss =126288.2421875\n",
      "step 8: d_loss =698498560.0 g_loss =775298.5\n",
      "step 9: d_loss =19330107392.0 g_loss =286383.9375\n",
      "\n",
      "Start epoch 1\n",
      "step 0: d_loss =4619820032.0 g_loss =219692.71875\n",
      "step 1: d_loss =9391558656.0 g_loss =42175.734375\n",
      "step 2: d_loss =19702935552.0 g_loss =204895.21875\n",
      "step 3: d_loss =36318609408.0 g_loss =173845.765625\n",
      "step 4: d_loss =12892940288.0 g_loss =311537.25\n",
      "step 5: d_loss =3764527616.0 g_loss =813815.125\n",
      "step 6: d_loss =3225978624.0 g_loss =1645239.875\n",
      "step 7: d_loss =4963955712.0 g_loss =1006329.8125\n",
      "step 8: d_loss =405736768.0 g_loss =883191.4375\n",
      "step 9: d_loss =2720139008.0 g_loss =1471283.25\n",
      "\n",
      "Start epoch 2\n",
      "step 0: d_loss =3928395008.0 g_loss =822645.625\n",
      "step 1: d_loss =1715702144.0 g_loss =433549.8125\n",
      "step 2: d_loss =1568941440.0 g_loss =12731180.0\n",
      "step 3: d_loss =2200448512.0 g_loss =19298110.0\n",
      "step 4: d_loss =1368787200.0 g_loss =14116409.0\n",
      "step 5: d_loss =9391723520.0 g_loss =4773572.0\n",
      "step 6: d_loss =640413760.0 g_loss =836580.5\n",
      "step 7: d_loss =6712108032.0 g_loss =2400.32275390625\n",
      "step 8: d_loss =3551595776.0 g_loss =2953682.25\n",
      "step 9: d_loss =4573419008.0 g_loss =7222374.0\n",
      "\n",
      "Start epoch 3\n",
      "step 0: d_loss =11568528384.0 g_loss =3221355.5\n",
      "step 1: d_loss =5239197696.0 g_loss =1218643.625\n",
      "step 2: d_loss =14175159296.0 g_loss =636942.375\n",
      "step 3: d_loss =13148618752.0 g_loss =486438.8125\n",
      "step 4: d_loss =11581559808.0 g_loss =87586.234375\n",
      "step 5: d_loss =5151353856.0 g_loss =12218992.0\n",
      "step 6: d_loss =3415265280.0 g_loss =58742696.0\n",
      "step 7: d_loss =423938883584.0 g_loss =4613327.5\n",
      "step 8: d_loss =5568216064.0 g_loss =1098501.875\n",
      "step 9: d_loss =8705742848.0 g_loss =0.0\n",
      "\n",
      "Start epoch 4\n",
      "step 0: d_loss =8880434176.0 g_loss =0.0\n",
      "step 1: d_loss =4517565440.0 g_loss =179626848.0\n",
      "step 2: d_loss =4443298816.0 g_loss =88468168.0\n",
      "step 3: d_loss =136635170816.0 g_loss =1728505600.0\n",
      "step 4: d_loss =11237079040.0 g_loss =8265259.5\n",
      "step 5: d_loss =13108487168.0 g_loss =79560720.0\n",
      "step 6: d_loss =23686402048.0 g_loss =19326846.0\n",
      "step 7: d_loss =28902385664.0 g_loss =22588892.0\n",
      "step 8: d_loss =34914435072.0 g_loss =15974022.0\n",
      "step 9: d_loss =38448844800.0 g_loss =12873040.0\n",
      "\n",
      "Start epoch 5\n",
      "step 0: d_loss =28652036096.0 g_loss =7172066.0\n",
      "step 1: d_loss =22536527872.0 g_loss =3374792.75\n",
      "step 2: d_loss =22633912320.0 g_loss =2678449.25\n",
      "step 3: d_loss =21657546752.0 g_loss =2509226.25\n",
      "step 4: d_loss =10171167744.0 g_loss =2767996.5\n",
      "step 5: d_loss =4207989248.0 g_loss =3361391.0\n",
      "step 6: d_loss =87489945600.0 g_loss =8121759.0\n",
      "step 7: d_loss =7262196736.0 g_loss =12771175.0\n",
      "step 8: d_loss =13119222784.0 g_loss =16310389.0\n",
      "step 9: d_loss =18947989504.0 g_loss =18421788.0\n",
      "\n",
      "Start epoch 6\n",
      "step 0: d_loss =20851965952.0 g_loss =19482314.0\n",
      "step 1: d_loss =27301765120.0 g_loss =19181010.0\n",
      "step 2: d_loss =20270061568.0 g_loss =18504982.0\n",
      "step 3: d_loss =28110342144.0 g_loss =17219952.0\n",
      "step 4: d_loss =19286433792.0 g_loss =15996222.0\n",
      "step 5: d_loss =12016646144.0 g_loss =14175126.0\n",
      "step 6: d_loss =10943328256.0 g_loss =12888271.0\n",
      "step 7: d_loss =4915864064.0 g_loss =11807750.0\n",
      "step 8: d_loss =4438693376.0 g_loss =10765064.0\n",
      "step 9: d_loss =3569720576.0 g_loss =10087842.0\n",
      "\n",
      "Start epoch 7\n",
      "step 0: d_loss =1401650432.0 g_loss =9753797.0\n",
      "step 1: d_loss =53674262528.0 g_loss =11677924.0\n",
      "step 2: d_loss =3429321984.0 g_loss =14301889.0\n",
      "step 3: d_loss =10830366720.0 g_loss =14496966.0\n",
      "step 4: d_loss =10737421312.0 g_loss =15745958.0\n",
      "step 5: d_loss =6553814016.0 g_loss =16328484.0\n",
      "step 6: d_loss =13747729408.0 g_loss =16195941.0\n",
      "step 7: d_loss =9697597440.0 g_loss =15522181.0\n",
      "step 8: d_loss =12161271808.0 g_loss =13925485.0\n",
      "step 9: d_loss =11568315392.0 g_loss =16844024.0\n",
      "\n",
      "Start epoch 8\n",
      "step 0: d_loss =16730320896.0 g_loss =28199860.0\n",
      "step 1: d_loss =14439934976.0 g_loss =22096120.0\n",
      "step 2: d_loss =12032751616.0 g_loss =64922328.0\n",
      "step 3: d_loss =13208862720.0 g_loss =168345536.0\n",
      "step 4: d_loss =6456281088.0 g_loss =230207584.0\n",
      "step 5: d_loss =6701768192.0 g_loss =272834176.0\n",
      "step 6: d_loss =6869703680.0 g_loss =83821656.0\n",
      "step 7: d_loss =4058424832.0 g_loss =294292.28125\n",
      "step 8: d_loss =4674326016.0 g_loss =7948452.0\n",
      "step 9: d_loss =4141171712.0 g_loss =1813886.375\n",
      "\n",
      "Start epoch 9\n",
      "step 0: d_loss =2203641088.0 g_loss =25739062.0\n",
      "step 1: d_loss =1282146176.0 g_loss =415249.5625\n",
      "step 2: d_loss =559028480.0 g_loss =818767.0625\n",
      "step 3: d_loss =40066039808.0 g_loss =5634675.5\n",
      "step 4: d_loss =2426171136.0 g_loss =59749460.0\n",
      "step 5: d_loss =1563356416.0 g_loss =79234880.0\n",
      "step 6: d_loss =3264447232.0 g_loss =284843872.0\n",
      "step 7: d_loss =4774217728.0 g_loss =317024.1875\n",
      "step 8: d_loss =4426955264.0 g_loss =43662.80859375\n",
      "step 9: d_loss =4862989824.0 g_loss =115818360.0\n",
      "\n",
      "Start epoch 10\n",
      "step 0: d_loss =4081663488.0 g_loss =16685725.0\n",
      "step 1: d_loss =4351385088.0 g_loss =2708330.75\n",
      "step 2: d_loss =2490409984.0 g_loss =0.0\n",
      "step 3: d_loss =4394984960.0 g_loss =6526529.0\n",
      "step 4: d_loss =2573289216.0 g_loss =804608.875\n",
      "step 5: d_loss =1750457216.0 g_loss =0.0\n",
      "step 6: d_loss =2745223680.0 g_loss =1518425.375\n",
      "step 7: d_loss =1417090304.0 g_loss =2975480.5\n",
      "step 8: d_loss =1408699264.0 g_loss =11193295.0\n",
      "step 9: d_loss =1031230464.0 g_loss =35385240.0\n",
      "\n",
      "Start epoch 11\n",
      "step 0: d_loss =6117946368.0 g_loss =19673372.0\n",
      "step 1: d_loss =2246711552.0 g_loss =41845232.0\n",
      "step 2: d_loss =3217676544.0 g_loss =22422306.0\n",
      "step 3: d_loss =7747361792.0 g_loss =10203636.0\n",
      "step 4: d_loss =6201469440.0 g_loss =5645161.0\n",
      "step 5: d_loss =7722414592.0 g_loss =949662.75\n",
      "step 6: d_loss =8678333440.0 g_loss =0.0\n",
      "step 7: d_loss =5200394752.0 g_loss =2982557.5\n",
      "step 8: d_loss =12394132480.0 g_loss =1465441.875\n",
      "step 9: d_loss =11057696768.0 g_loss =30754710.0\n",
      "\n",
      "Start epoch 12\n",
      "step 0: d_loss =8668549120.0 g_loss =62660348.0\n",
      "step 1: d_loss =11945548800.0 g_loss =544422656.0\n",
      "step 2: d_loss =6628768256.0 g_loss =5984198.0\n",
      "step 3: d_loss =10829288448.0 g_loss =410062912.0\n",
      "step 4: d_loss =10752455680.0 g_loss =164336832.0\n",
      "step 5: d_loss =6117380608.0 g_loss =28709658.0\n",
      "step 6: d_loss =6413210112.0 g_loss =146003280.0\n",
      "step 7: d_loss =3123325184.0 g_loss =74566032.0\n",
      "step 8: d_loss =2925163776.0 g_loss =1252357632.0\n",
      "step 9: d_loss =2409856512.0 g_loss =67416152.0\n",
      "\n",
      "Start epoch 13\n",
      "step 0: d_loss =454144608.0 g_loss =172519808.0\n",
      "step 1: d_loss =75183947776.0 g_loss =31788938.0\n",
      "step 2: d_loss =1958450944.0 g_loss =13551550.0\n",
      "step 3: d_loss =6689092608.0 g_loss =2774763.25\n",
      "step 4: d_loss =6588686848.0 g_loss =527470.5625\n",
      "step 5: d_loss =5403530752.0 g_loss =155822112.0\n",
      "step 6: d_loss =9374495744.0 g_loss =1917504.875\n",
      "step 7: d_loss =10067827712.0 g_loss =7124596.5\n",
      "step 8: d_loss =7996436480.0 g_loss =2478000.0\n",
      "step 9: d_loss =9585787904.0 g_loss =50125364.0\n",
      "\n",
      "Start epoch 14\n",
      "step 0: d_loss =12020154368.0 g_loss =14291848.0\n",
      "step 1: d_loss =8991365120.0 g_loss =46878152.0\n",
      "step 2: d_loss =10133694464.0 g_loss =12662353.0\n",
      "step 3: d_loss =10941974528.0 g_loss =21161366.0\n",
      "step 4: d_loss =7014308352.0 g_loss =37993288.0\n",
      "step 5: d_loss =4427783168.0 g_loss =24562464.0\n",
      "step 6: d_loss =10009576448.0 g_loss =28893518.0\n",
      "step 7: d_loss =4598776832.0 g_loss =18601116.0\n",
      "step 8: d_loss =4676076032.0 g_loss =27996692.0\n",
      "step 9: d_loss =5445304832.0 g_loss =34364112.0\n"
     ]
    }
   ],
   "source": [
    "test_optimizers(\n",
    "    keras.optimizers.Adam(learning_rate=0.03,clipnorm=1),keras.optimizers.Adam(learning_rate=0.03,clipnorm=1),epochs=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unexpected keyword argument passed to optimizer: epochs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-8c47163d2c9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m test_optimizers(\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0003\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclipnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0003\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclipnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m )\n",
      "\u001b[1;32mc:\\users\\jlbak\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, name, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \"\"\"\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_hyper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_hyper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'decay'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jlbak\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 268\u001b[1;33m         raise TypeError(\"Unexpected keyword argument \"\n\u001b[0m\u001b[0;32m    269\u001b[0m                         \"passed to optimizer: \" + str(k))\n\u001b[0;32m    270\u001b[0m       \u001b[1;31m# checks that all keyword arguments are non-negative.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected keyword argument passed to optimizer: epochs"
     ]
    }
   ],
   "source": [
    "test_optimizers(\n",
    "    keras.optimizers.Adam(learning_rate=0.0003,clipnorm=1),keras.optimizers.Adam(learning_rate=0.0003,clipnorm=1,epochs=15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANoneGen(keras.Model):\n",
    "    def __init__(self, discriminators, generators, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminators = discriminators\n",
    "        self.generators = generators\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, binary_loss_fn,multiclass_loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.binary_loss_fn = binary_loss_fn\n",
    "        self.multiclass_loss_fn = multiclass_loss_fn\n",
    "\n",
    "    def train_step(self, labels_multiclass,matrices,batch_size):\n",
    "        #fake images\n",
    "        layer_loss=[]\n",
    "        net_d_loss=0\n",
    "        net_g_loss=0\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        # Decode them to fake images\n",
    "        #generated_images = self.generator(random_latent_vectors)\n",
    "        ret={}\n",
    "        updated_g_vars=0\n",
    "        for real_mats,block,discriminatorin zip(matrices,style_blocks,self.discriminators):\n",
    "            gen_mats=generator(random_latent_vectors)\n",
    "            combined_mats = tf.concat([gen_mats, real_mats], axis=0)\n",
    "            # Assemble labels discriminating real from fake images\n",
    "            labels_binary = tf.concat(\n",
    "                [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "            )\n",
    "            # Add random noise to the labels - important trick!\n",
    "            labels_binary += 0.05 * tf.random.uniform(tf.shape(labels_binary))\n",
    "            # Train the discriminator\n",
    "            with tf.GradientTape() as tape:\n",
    "                #pred_binary,pred_multiclass = discriminator(combined_mats)\n",
    "                pred_binary= discriminator(combined_mats)\n",
    "                #pred_multiclass=pred_multiclass[-len(real_mats):]\n",
    "                d_loss = self.binary_loss_fn(labels_binary, pred_binary) #+self.multiclass_loss_fn(labels_multiclass,pred_multiclass)\n",
    "            grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(grads, discriminator.trainable_weights)\n",
    "            )\n",
    "            \n",
    "            #random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            # Assemble labels that say \"all real images\"\n",
    "            misleading_labels = tf.zeros((batch_size, 1))\n",
    "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "            # Train the generator\n",
    "            #g_mats = tf.Variable([func(vector)[0] for vector in random_latent_vectors])\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = discriminator(generator(random_latent_vectors))\n",
    "                g_loss = self.binary_loss_fn( misleading_labels,predictions)\n",
    "            grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "            #print([sum(grad) for grad in grads])\n",
    "            for g in range(updated_g_vars):\n",
    "                grads[g]=tf.zeros(shape=grads[g].shape)\n",
    "            updated_g_vars=len(grads)\n",
    "            self.g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "            \n",
    "            net_d_loss+=d_loss\n",
    "            net_g_loss+=g_loss\n",
    "            layer_loss.append({'g_loss':g_loss,'d_loss':d_loss})\n",
    "        return layer_loss,net_g_loss,net_d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
